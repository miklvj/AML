{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - classification\n",
    "\n",
    "Hi there! In this assignment, you will use a CNN to solve an adapted Question 2 of the winter 2023 exam in applied machine learning:\n",
    "\n",
    "As in Assignment 1 and 2, the primary objective of this exam is to perform image classification using the PCam dataset. For a detailed description of the dataset, please refer to the assignment 1 description. The assignment is posted as a Kaggle competition and is available here: https://www.kaggle.com/t/cda2949c5097437581cdb9abd32091ae\n",
    "\n",
    "To get you started, I have provided a complete working example, which is decent but not very impressive.\n",
    "\n",
    "When you are done, submit your results on the Kaggle webpage for this competition. If you do not like to show your score to everyone, you may use an anonymous username on Kaggle.\n",
    "\n",
    "However, I suggest you use your real name, after all it is just meant as an exercise and it is more fun that way. You can submit 5 times every day, so you can experiment with some stuff without being \"locked in\".\n",
    "\n",
    "The assignment on Kaggle can be accessed here: https://www.kaggle.com/t/44e8836b5ee5433c81c94136b54938a8\n",
    "\n",
    "# Details\n",
    "\n",
    "The metric used to score this assignment is accuracy (as in the first assignment).\n",
    "\n",
    "### Question (adapted from the exam):\n",
    "Use deep learning to perform image classification (tumor detection). Specifically, you must train CNNs to solve the problem. Here you must explicitly:\n",
    "1. Consider alternative CNN-model architectures.\n",
    "2. Consider different optimization methods.\n",
    "3. Consider regularization such as dropout, weight regularization, or early stopping. \n",
    "4. Consider data augmentation impacts the training of your model. Here, be sure to visualize plots of train and validation losses and accuracies both with and without the use of data augmentation. \n",
    "5. Consider transfer learning.\n",
    "\n",
    "**Note:** When you do hyperparameter tuning, you should use the validation set. The test set should only be used for the final evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints to get you started (with a very simple model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that takes a (None,96,96,3) array and turn it into (None, 32,32,1) (grayscale, resize and normalize). This function might also become handy if the original images are too large for your hardware configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_normalize_image(image):\n",
    "    image = tf.image.resize(image,[32,32])\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    return image / 255.0\n",
    "\n",
    "def convert_sample(data):\n",
    "\n",
    "# Create a TensorFlow dataset from the training data features\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "# Define a function to resize each image in the dataset\n",
    "\n",
    "# Apply the resize function to each image in the dataset\n",
    "    resized_dataset = dataset.map(resize_and_normalize_image)\n",
    "\n",
    "# Convert the resized dataset to a NumPy array\n",
    "    resized_arr = np.array(list(resized_dataset.as_numpy_iterator()))\n",
    "\n",
    "    return resized_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the raw training data: (26214, 96, 96, 3)\n",
      "Shape of the raw test data: (1638, 96, 96, 3)\n",
      "Shape the resized training data: (26214, 32, 32, 1)\n",
      "Shape the resized test data: (1638, 32, 32, 1)\n",
      "Shape of the raw labels: (26214, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the training data features\n",
    "#X_train_raw = np.load('/mnt/c/Users/cmd/Dropbox/Teaching/amlFall2023/assignments/Xtrain.npy')\n",
    "X_train_raw = np.load('Xtrain.npy')\n",
    "print(f'Shape of the raw training data: {X_train_raw.shape}')\n",
    "#X_test_raw = np.load('/mnt/c/Users/cmd/Dropbox/Teaching/amlFall2023/assignments/Xtest.npy')\n",
    "X_test_raw = np.load('Xtest.npy')\n",
    "print(f'Shape of the raw test data: {X_test_raw.shape}')\n",
    "\n",
    "X_train = convert_sample(X_train_raw)\n",
    "print(f'Shape the resized training data: {X_train.shape}')\n",
    "\n",
    "X_test = convert_sample(X_test_raw)\n",
    "print(f'Shape the resized test data: {X_test.shape}')\n",
    "\n",
    "#y_raw = np.load('/mnt/c/Users/cmd/Dropbox/Teaching/amlFall2023/assignments/ytrain.npy')\n",
    "y_raw = np.load('ytrain.npy')\n",
    "y_raw = y_raw.reshape(-1,1) \n",
    "print(f'Shape of the raw labels: {y_raw.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 32, 32, 1) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define the batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Create a tf.data.Dataset object from the training data and labels\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_raw))\n",
    "\n",
    "# Batch the training data\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Print the shape of the batches\n",
    "for batch in train_dataset:\n",
    "    print(batch[0].shape, batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "6554/6554 [==============================] - 30s 4ms/step - loss: 0.6874 - accuracy: 0.5582\n",
      "Epoch 2/2\n",
      "6554/6554 [==============================] - 29s 4ms/step - loss: 0.5879 - accuracy: 0.6910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f080027370>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu',padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu',padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu',padding='same'),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "sgd_opt = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(optimizer=sgd_opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code makes predictions and then saves them (after checking they are in correct format).\n",
    "\n",
    "The argmax converts probabilities to specific class predictions.\n",
    "\n",
    "And finally convert to appropriate $\\texttt{.csv}$ for Kaggle submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_hat = model.predict(X_test)\n",
    "y_test_hat = np.argmax(y_test_hat, axis=1)\n",
    "\n",
    "ytest_hat_pd = pd.DataFrame({\n",
    "    'Id': list(range(len(y_test_hat))),\n",
    "    'Predicted': y_test_hat.reshape(-1,),\n",
    "})\n",
    "\n",
    "ytest_hat_pd.to_csv('y_test_hat_cnn.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('amlfall22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d3cedec8935a2c28d6fd602c3007747750e2af1c4c937c29fac0d323bf1b544b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
