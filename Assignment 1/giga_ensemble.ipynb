{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sample(image):\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image = tf.image.resize(image,[96,96]).numpy()\n",
    "    image = image.reshape(1,-1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26214, 96, 96, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\amlfall23\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:591: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data features (observations,features): (26214, 9216)\n",
      "Shape of training data labels (observations,): (26214,)\n",
      "Shape of training data features (observations,features): (1638, 9216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\amlfall23\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:591: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = np.load('Xtrain.npy')\n",
    "print(X.shape)\n",
    "X = np.vstack(list(map(convert_sample,X)))\n",
    "X = StandardScaler(with_mean=0, with_std=1).fit_transform(X)\n",
    "print(f'Shape of training data features (observations,features): {X.shape}')\n",
    "\n",
    "y = np.load('ytrain.npy')\n",
    "y = y.reshape(-1,)    \n",
    "print(f'Shape of training data labels (observations,): {y.shape}')\n",
    "\n",
    "Xtest = np.load('Xtest.npy')\n",
    "Xtest = np.vstack(list(map(convert_sample,Xtest)))\n",
    "Xtest = StandardScaler(with_mean=0, with_std=1).fit_transform(Xtest)\n",
    "print(f'Shape of training data features (observations,features): {Xtest.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(26000)\n",
    "\n",
    "subset1_indices = indices[:5200]\n",
    "subset2_indices = indices[5200:10400]\n",
    "subset3_indices = indices[10400:15600]\n",
    "subset4_indices = indices[15600:20800]\n",
    "subset5_indices = indices[20800:26000]\n",
    "\n",
    "X_subset1, y_subset1 = X[subset1_indices], y[subset1_indices]\n",
    "X_subset2, y_subset2 = X[subset2_indices], y[subset2_indices]\n",
    "X_subset3, y_subset3 = X[subset3_indices], y[subset3_indices]\n",
    "X_subset4, y_subset4 = X[subset4_indices], y[subset4_indices]\n",
    "X_subset5, y_subset5 = X[subset5_indices], y[subset5_indices]\n",
    "\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(X_subset1, y_subset1, test_size=0.2, random_state=42)\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(X_subset2, y_subset2, test_size=0.2, random_state=42)\n",
    "X_train3, X_val3, y_train3, y_val3 = train_test_split(X_subset3, y_subset3, test_size=0.2, random_state=42)\n",
    "X_train4, X_val4, y_train4, y_val4 = train_test_split(X_subset4, y_subset4, test_size=0.2, random_state=42)\n",
    "X_train5, X_val5, y_train5, y_val5 = train_test_split(X_subset5, y_subset5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best1 = svm.SVC(kernel='rbf', C = 1.0)\n",
    "\n",
    "svm_best1.fit(np.concatenate([X_train1, X_val1]), np.concatenate([y_train1, y_val1]))\n",
    "\n",
    "ytest_hat_svm1 = svm_best1.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best2 = svm.SVC(kernel='rbf', C = 1.0)\n",
    "\n",
    "svm_best2.fit(np.concatenate([X_train2, X_val2]), np.concatenate([y_train2, y_val2]))\n",
    "\n",
    "ytest_hat_svm2 = svm_best2.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best3 = svm.SVC(kernel='rbf', C = 1.0)\n",
    "\n",
    "svm_best3.fit(np.concatenate([X_train3, X_val3]), np.concatenate([y_train3, y_val3]))\n",
    "\n",
    "ytest_hat_svm3 = svm_best3.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best4 = svm.SVC(kernel='rbf', C = 1.0)\n",
    "\n",
    "svm_best4.fit(np.concatenate([X_train4, X_val4]), np.concatenate([y_train4, y_val4]))\n",
    "\n",
    "ytest_hat_svm4 = svm_best4.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best5 = svm.SVC(kernel='rbf', C = 1.0)\n",
    "\n",
    "svm_best5.fit(np.concatenate([X_train5, X_val5]), np.concatenate([y_train5, y_val5]))\n",
    "\n",
    "ytest_hat_svm5 = svm_best5.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best1 = ensemble.RandomForestClassifier(\n",
    "    max_depth = None, \n",
    "    n_estimators = 1000, \n",
    "    min_samples_split = 33,\n",
    "    min_samples_leaf = 5,\n",
    "    max_features=\"sqrt\"\n",
    ")\n",
    "\n",
    "rf_best1.fit(np.concatenate([X_train1, X_val1]), np.concatenate([y_train1, y_val1]))\n",
    "\n",
    "ytest_hat_rf1 = rf_best1.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best2 = ensemble.RandomForestClassifier(\n",
    "    max_depth = None, \n",
    "    n_estimators = 1000, \n",
    "    min_samples_split = 33,\n",
    "    min_samples_leaf = 5,\n",
    "    max_features=\"sqrt\"\n",
    ")\n",
    "\n",
    "rf_best2.fit(np.concatenate([X_train2, X_val2]), np.concatenate([y_train2, y_val2]))\n",
    "\n",
    "ytest_hat_rf2 = rf_best2.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best3 = ensemble.RandomForestClassifier(\n",
    "    max_depth = None, \n",
    "    n_estimators = 1000, \n",
    "    min_samples_split = 33,\n",
    "    min_samples_leaf = 5,\n",
    "    max_features=\"sqrt\"\n",
    ")\n",
    "\n",
    "rf_best3.fit(np.concatenate([X_train3, X_val3]), np.concatenate([y_train3, y_val3]))\n",
    "\n",
    "ytest_hat_rf3 = rf_best3.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best4 = ensemble.RandomForestClassifier(\n",
    "    max_depth = None, \n",
    "    n_estimators = 1000, \n",
    "    min_samples_split = 33,\n",
    "    min_samples_leaf = 5,\n",
    "    max_features=\"sqrt\"\n",
    ")\n",
    "\n",
    "rf_best4.fit(np.concatenate([X_train4, X_val4]), np.concatenate([y_train4, y_val4]))\n",
    "\n",
    "ytest_hat_rf4 = rf_best4.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best5 = ensemble.RandomForestClassifier(\n",
    "    max_depth = None, \n",
    "    n_estimators = 1000, \n",
    "    min_samples_split = 33,\n",
    "    min_samples_leaf = 5,\n",
    "    max_features=\"sqrt\"\n",
    ")\n",
    "\n",
    "rf_best5.fit(np.concatenate([X_train5, X_val5]), np.concatenate([y_train5, y_val5]))\n",
    "\n",
    "ytest_hat_rf5 = rf_best5.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_best1 = GradientBoostingClassifier(n_estimators = 800, min_samples_split=10, min_samples_leaf=2, max_depth=2, learning_rate=0.15)\n",
    "gb_best1.fit(np.concatenate([X_train1, X_val1]), np.concatenate([y_train1, y_val1]))\n",
    "\n",
    "ytest_hat_gb1 = gb_best1.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_best2 = GradientBoostingClassifier(n_estimators = 800, min_samples_split=10, min_samples_leaf=2, max_depth=2, learning_rate=0.15)\n",
    "gb_best2.fit(np.concatenate([X_train2, X_val2]), np.concatenate([y_train2, y_val2]))\n",
    "\n",
    "ytest_hat_gb2 = gb_best2.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_best3 = GradientBoostingClassifier(n_estimators = 800, min_samples_split=10, min_samples_leaf=2, max_depth=2, learning_rate=0.15)\n",
    "gb_best3.fit(np.concatenate([X_train3, X_val3]), np.concatenate([y_train3, y_val3]))\n",
    "\n",
    "ytest_hat_gb3 = gb_best3.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_best4 = GradientBoostingClassifier(n_estimators = 800, min_samples_split=10, min_samples_leaf=2, max_depth=2, learning_rate=0.15)\n",
    "gb_best4.fit(np.concatenate([X_train4, X_val4]), np.concatenate([y_train4, y_val4]))\n",
    "\n",
    "ytest_hat_gb4 = gb_best4.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_best5 = GradientBoostingClassifier(n_estimators = 800, min_samples_split=10, min_samples_leaf=2, max_depth=2, learning_rate=0.15)\n",
    "gb_best5.fit(np.concatenate([X_train5, X_val5]), np.concatenate([y_train5, y_val5]))\n",
    "\n",
    "ytest_hat_gb5 = gb_best5.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_combined1 = np.c_[ytest_hat_gb1, ytest_hat_rf1, ytest_hat_svm1]\n",
    "y_test_hat_combined1 = np.round(np.sum(y_test_hat_combined1, axis=1) / y_test_hat_combined1.shape[1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_combined2 = np.c_[ytest_hat_gb2, ytest_hat_rf2, ytest_hat_svm2]\n",
    "y_test_hat_combined2 = np.round(np.sum(y_test_hat_combined2, axis=1) / y_test_hat_combined2.shape[1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_combined3 = np.c_[ytest_hat_gb3, ytest_hat_rf3, ytest_hat_svm3]\n",
    "y_test_hat_combined3 = np.round(np.sum(y_test_hat_combined3, axis=1) / y_test_hat_combined3.shape[1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_combined4 = np.c_[ytest_hat_gb4, ytest_hat_rf4, ytest_hat_svm4]\n",
    "y_test_hat_combined4 = np.round(np.sum(y_test_hat_combined4, axis=1) / y_test_hat_combined4.shape[1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_combined5 = np.c_[ytest_hat_gb5, ytest_hat_rf5, ytest_hat_svm5]\n",
    "y_test_hat_combined5 = np.round(np.sum(y_test_hat_combined5, axis=1) / y_test_hat_combined5.shape[1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_combined = np.c_[y_test_hat_combined1, y_test_hat_combined2, y_test_hat_combined3, y_test_hat_combined4, y_test_hat_combined5]\n",
    "y_test_hat_combined = np.round(np.sum(y_test_hat_combined, axis=1) / y_test_hat_combined.shape[1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_combined= pd.DataFrame({\n",
    "    'Id': list(range(len(y_test_hat_combined))),\n",
    "    'Predicted': y_test_hat_combined.reshape(-1,),\n",
    "})\n",
    "y_test_hat_combined.to_csv(\"ytest_hat.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlfall23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
