{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pub.towardsai.net/whirlwind-tour-of-rnns-a11effb7808f\n",
    "\n",
    "The number of hidden nodes (nb_nodes) is a hyperparameter:\n",
    "- A high number might result in overfitting...\n",
    "- Try out different amount of hidden nodes - as always\n",
    "- The number of nodes should ideally be in line with the complexity and dimensionality of your input data. For high-dimensional data, you might need more nodes to effectively capture the data's structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### RNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-version 2.10.1\n"
     ]
    }
   ],
   "source": [
    "print(f'tf-version {tf.__version__}')\n",
    "nb_input_features = 10000\n",
    "nb_timesteps = 6\n",
    "nb_nodes = 6\n",
    "\n",
    "simple_rnn_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(nb_nodes, \n",
    "                              input_shape=(nb_timesteps, nb_input_features),\n",
    "                              return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(units=nb_timesteps, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 4)                 240       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 240\n",
      "Trainable params: 240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of trainable parameters = 240\n"
     ]
    }
   ],
   "source": [
    "nb_input_features = 10\n",
    "nb_timesteps = 5\n",
    "nb_nodes = 4\n",
    "\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(nb_nodes, \n",
    "                         input_shape=(nb_timesteps, nb_input_features))\n",
    "])\n",
    "\n",
    "lstm_model.summary()\n",
    "\n",
    "print(f'Number of trainable parameters = {4 * (nb_input_features + nb_nodes + 1) * nb_nodes}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 4)\n",
      "(32, 5, 4)\n",
      "(32, 4)\n"
     ]
    }
   ],
   "source": [
    "nb_input_features = 10\n",
    "nb_timesteps = 5\n",
    "nb_nodes = 4\n",
    "batch=32\n",
    "inputs = tf.random.normal([batch, nb_timesteps, nb_input_features])\n",
    "gru = tf.keras.layers.GRU(units= nb_nodes)\n",
    "output = gru(inputs)\n",
    "print(output.shape)\n",
    "\n",
    "gru = tf.keras.layers.GRU(4, return_sequences=True, return_state=True)\n",
    "whole_sequence_output, final_state = gru(inputs)\n",
    "print(whole_sequence_output.shape)\n",
    "\n",
    "print(final_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_2 (GRU)                 (None, 4)                 192       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 192\n",
      "Trainable params: 192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of trainable parameters = 192\n"
     ]
    }
   ],
   "source": [
    "nb_input_features = 10\n",
    "nb_timesteps = 5\n",
    "nb_nodes = 4\n",
    "\n",
    "gru_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GRU(nb_nodes, \n",
    "                        input_shape=(nb_timesteps, nb_input_features))\n",
    "]) # depends on whether reset_after is True or False! If False, will substract 3 * nb_nodes parameters.\n",
    "\n",
    "gru_model.summary()\n",
    "\n",
    "print(f'Number of trainable parameters = {3 * (nb_input_features + nb_nodes + 2) * nb_nodes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidirectional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 8)                480       \n",
      " l)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 480\n",
      "Trainable params: 480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of trainable parameters = 480\n"
     ]
    }
   ],
   "source": [
    "nb_input_features = 10\n",
    "nb_timesteps = 5\n",
    "nb_nodes = 4\n",
    "\n",
    "bidirectional_concat_lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(nb_nodes), \n",
    "                                  input_shape=(nb_timesteps,\n",
    "                                               nb_input_features)), \n",
    "])\n",
    "\n",
    "bidirectional_concat_lstm_model.summary()\n",
    "\n",
    "print(f'Number of trainable parameters = {2 * 4 * (nb_input_features + nb_nodes + 1) * nb_nodes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One to One:\n",
    "\n",
    "Description: This is the simplest form, where there is one input and one output. It's essentially a standard neural network structure applied to a single data point.\n",
    "Example: A classic use case is a standard classification problem where you predict a single label from a single data point.\n",
    "\n",
    "##### One to Many:\n",
    "\n",
    "Description: In this architecture, a single input generates a sequence of outputs.\n",
    "Example: An example is image captioning, where an image (single input) is used to generate a sequence of words forming a caption (many outputs).\n",
    "Many to One:\n",
    "\n",
    "Description: Here, a sequence of inputs leads to a single output. This is commonly used in tasks where the context or sequence is crucial for making a single prediction.\n",
    "Example: Sentiment analysis is a typical application. A sequence of words (many inputs) is used to determine the sentiment of the sentence (one output).\n",
    "\n",
    "##### Many to Many:\n",
    "\n",
    "Description: This type involves a sequence of inputs and a sequence of outputs. There are two subtypes:\n",
    "\n",
    "Synced Many to Many: The output is synchronized with the input at each timestep.\n",
    "\n",
    "Async Many to Many: The input and output sequences are not synchronized.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Synced: Part-of-speech tagging, where each word in a sentence (many inputs) is tagged with a part-of-speech label (many outputs), with each output corresponding to each input.\n",
    "\n",
    "Async: Machine translation, where a sentence in one language (many inputs) is translated into another language (many outputs), but the lengths of the input and output sequences can be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "his is done by setting the argument **return_sequences** of a recurrent layer to **True**. As such, this needs to be done for each recurrent layer aside from the last (where we are only interested in the last time step, and as such no longer need the entire sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 5, 4)              240       \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 5, 3)              81        \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 4)                24        \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 350\n",
      "Trainable params: 350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nb_input_features = 10\n",
    "nb_timesteps = 5;   nb_nodes_1 = 4; nb_nodes_2 = 3; nb_nodes_3 = 2\n",
    "\n",
    "deep_rnn_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(nb_nodes_1, input_shape=(nb_timesteps, nb_input_features), return_sequences=True),\n",
    "    tf.keras.layers.GRU(nb_nodes_2, return_sequences=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(nb_nodes_3)),\n",
    "    tf.keras.layers.Dense(1), # maybe we want to perform regression, where this might be the final layer\n",
    "])\n",
    "\n",
    "deep_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-grams of words (bag of words)\n",
    "\n",
    "The main idea of using n-grams in a bag-of-words model for text prediction is to capture the frequency and context of word sequences in the text data. In this approach:\n",
    "\n",
    "N-grams: These are sequences of 'n' consecutive words. For example, in a bigram (2-gram) model, you consider pairs of adjacent words. This helps in capturing some context and word order, unlike a single word (unigram) model.\n",
    "\n",
    "Bag-of-Words: This model treats text as an unordered collection (or \"bag\") of words. It ignores grammar and word order, focusing only on the occurrence of words in the document.\n",
    "\n",
    "When combined, n-grams in a bag-of-words framework provide a simple yet effective way of predicting text. The model can predict or classify new text based on the frequency and patterns of n-grams seen during training. It's a foundational technique in natural language processing used for tasks like language modeling, text classification, and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vacabulary: 11\n",
      "vocabulary: ['[UNK]' 'the' 'the mat' 'the cat' 'sat on' 'sat' 'on the' 'on' 'mat'\n",
      " 'cat sat' 'cat']\n",
      "[UNK]\n",
      "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "the\n",
      "[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "the mat\n",
      "[0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "the cat\n",
      "[0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "sat on\n",
      "[0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "sat\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "on the\n",
      "[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
      "on\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "mat\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "cat sat\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n",
      "cat\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "text    = [\"the cat sat on the mat\"]\n",
    "encoder = tf.keras.layers.TextVectorization(ngrams=2,max_tokens=100,\n",
    "                                            output_mode=\"multi_hot\")\n",
    "encoder.adapt(text) # Computes a vocabulary of string terms from tokens in a dataset.\n",
    "vocab   = np.array(encoder.get_vocabulary()) # Get and print the vocabulary\n",
    "print(f'length of vacabulary: {len(vocab)}')\n",
    "print(f'vocabulary: {vocab}')\n",
    "encoded_example = []\n",
    "for ngram in vocab:\n",
    "    print(ngram)\n",
    "    print(list(encoder(ngram).numpy()))\n",
    "    encoded_example.append(list(encoder(ngram).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming the bag of words to a matrix:\n",
      "\n",
      "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "[0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print('transforming the bag of words to a matrix:\\n', \n",
    "      *encoded_example,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing\n",
    "\n",
    "So here we create our numerical values for our vocabulary but unlike n-grams we get a code for - not pairwise words - but for each single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "one_hot_dict = {\n",
    "    'the': 0,\n",
    "    'cat': 1,\n",
    "    'sat': 2,\n",
    "    'on': 3,\n",
    "    'mat': 4,\n",
    "}\n",
    "\n",
    "numerical_encoded_sentence = [one_hot_dict[word] \n",
    "                              for word in 'the cat sat on the mat'.split(' ')]\n",
    "print(numerical_encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.utils.to_categorical(numerical_encoded_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings\n",
    "\n",
    "##### Dimensionality Reduction:\n",
    "\n",
    "Traditional one-hot encoding results in high-dimensional vectors (length equals the size of the vocabulary) which are mostly zeros, leading to sparsity and inefficiency.\n",
    "Embeddings, on the other hand, represent words in a lower-dimensional continuous space, typically ranging from 50 to 300 dimensions. This significantly reduces the size of the representation.\n",
    "\n",
    "##### Capturing Semantic Information:\n",
    "\n",
    "Unlike one-hot vectors, embeddings are designed to capture semantic information, meaning that words with similar meanings are represented by similar vectors. This is a powerful feature that n-grams and hashing techniques do not inherently provide.\n",
    "For example, in a well-trained embedding space, words like 'king' and 'queen' would have vectors that are close to each other.\n",
    "\n",
    "##### Training:\n",
    "\n",
    "Embeddings are learned from data, often using neural networks. Models like Word2Vec, GloVe, or FastText analyze the contexts in which words appear and use this information to construct the embedding space.\n",
    "This learning process allows embeddings to capture subtle semantic and syntactic relationships between words.\n",
    "\n",
    "##### Distinction from N-Grams:\n",
    "\n",
    "N-grams capture local context and word order by treating sequences of words as single units. However, they don't inherently capture semantic similarity between words and result in high-dimensional, sparse representations.\n",
    "Embeddings focus more on the semantic relationships and provide dense, lower-dimensional representations.\n",
    "\n",
    "##### Distinction from Hashing:\n",
    "\n",
    "Hashing techniques (like hash-trick in feature engineering) are used to handle large vocabularies efficiently but typically involve loss of information and do not account for semantics.\n",
    "Word embeddings, in contrast, are a more nuanced way of representing text, preserving and leveraging semantic information.\n",
    "In essence, embeddings represent a significant advancement in the field of NLP, offering a way to efficiently and effectively capture the nuances of language, far beyond the capabilities of older techniques like n-grams or hashing.\n",
    "\n",
    "##### Hashing and N-Grams:\n",
    "\n",
    "Frequency Focus: \n",
    "\n",
    "Both hashing and n-grams primarily capture the frequencies of word occurrences or sequences. N-grams consider the local order of words, offering a bit more context than individual word frequencies.\n",
    "Sparsity and Dimensionality: These methods often lead to high-dimensional, sparse representations. In large vocabularies, this can result in memory inefficiency and computational challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Potential Collisions:\n",
    "\n",
    "Collisions occur if different words are assigned the same integer. With a vocabulary size of 10, if your dataset has more than 10 unique words, there's a high chance of collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['nice food',\n",
    "        'amazing restaurant',\n",
    "        'too good',\n",
    "        'just loved it!',\n",
    "        'will go again',\n",
    "        'horrible food',\n",
    "        'never go there',\n",
    "        'poor service',\n",
    "        'poor quality',\n",
    "        'needs improvement']\n",
    "sentiment = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2]\n",
      "[3, 3]\n",
      "Is there a problem?...collisions?\n",
      "\n",
      "Trying with a larger vocabulary\n",
      "[3, 8]\n",
      "[9, 6]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 4\n",
    "print(one_hot(\"nice food\",vocabulary_size))\n",
    "print(one_hot(\"amazing restaurant\",vocabulary_size))\n",
    "print(\"Is there a problem?...collisions?\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Trying with a larger vocabulary\")\n",
    "vocabulary_size = 10\n",
    "print(one_hot(\"nice food\",vocabulary_size))\n",
    "print(one_hot(\"amazing restaurant\",vocabulary_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vector representation of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot encoding/hashing: [[3, 8], [9, 6], [8, 6], [9, 1, 5], [2, 1, 3], [5, 8], [1, 1, 5], [3, 4], [3, 5], [5, 2]]\n",
      "Any collisions?\n",
      "one hot encoding/hashing: [[3 8 0 0]\n",
      " [9 6 0 0]\n",
      " [8 6 0 0]\n",
      " [9 1 5 0]\n",
      " [2 1 3 0]\n",
      " [5 8 0 0]\n",
      " [1 1 5 0]\n",
      " [3 4 0 0]\n",
      " [3 5 0 0]\n",
      " [5 2 0 0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_reviews = [one_hot(d, vocabulary_size) for d in reviews]\n",
    "print(f'one hot encoding/hashing: {encoded_reviews}')\n",
    "print(\"Any collisions?\")\n",
    "\n",
    "max_length = 4\n",
    "padded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\n",
    "print(f'one hot encoding/hashing: {padded_reviews}')\n",
    "\n",
    "#Note that there can be a \"collision\": Some words are encoded with the same integer!!\n",
    "#Increasing the vocabulary will reduce the likelihood of a collision...but what are\n",
    "#the effects of this downstream?..a lager embedding layer/matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach that eliminate collisions without increasing the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vacabulary: 22\n",
      "one hot encoding/hashing: ['nice food', 'amazing restaurant', 'too good', 'just loved it!', 'will go again', 'horrible food', 'never go there', 'poor service', 'poor quality', 'needs improvement']\n",
      "one hot encoding/hashing:\n",
      " [[11  4  0  0]\n",
      " [20  9  0  0]\n",
      " [ 6 19  0  0]\n",
      " [15 14 16  0]\n",
      " [ 5  3 21  0]\n",
      " [18  4  0  0]\n",
      " [12  3  7  0]\n",
      " [ 2  8  0  0]\n",
      " [ 2 10  0  0]\n",
      " [13 17  0  0]]\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 40\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_VOCAB_SIZE)\n",
    "encoder.adapt(reviews)\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "print(f'length of vacabulary: {len(vocab)}')\n",
    "\n",
    "encoded_example = encoder(reviews).numpy()\n",
    "max_length = 4\n",
    "padded_reviews = pad_sequences(encoded_example, maxlen=max_length,\n",
    "                               padding='post')\n",
    "print(f'one hot encoding/hashing: {reviews}')\n",
    "print(f'one hot encoding/hashing:\\n {padded_reviews}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bulding a simple model with an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 5)              110       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 131\n",
      "Trainable params: 131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dimension = 5\n",
    "embedding_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(vocab), output_dim=embedding_dimension,\n",
    "                             input_length=max_length,name=\"embedding\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "embedding_model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                        metrics=['accuracy'])\n",
    "print(embedding_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining input and output and fitting/evaluating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_reviews\n",
    "y = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 532ms/step - loss: 0.6865 - accuracy: 0.6000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6850 - accuracy: 0.6000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6836 - accuracy: 0.6000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6821 - accuracy: 0.6000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6806 - accuracy: 0.6000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6791 - accuracy: 0.6000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6776 - accuracy: 0.8000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6762 - accuracy: 0.8000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6747 - accuracy: 0.8000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6732 - accuracy: 0.8000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6717 - accuracy: 0.8000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6702 - accuracy: 0.8000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6687 - accuracy: 0.8000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6672 - accuracy: 0.8000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6657 - accuracy: 0.8000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6642 - accuracy: 0.8000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6627 - accuracy: 0.9000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6611 - accuracy: 0.9000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6596 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6581 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6565 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6549 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6534 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6518 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6502 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6486 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6470 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6454 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6438 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6421 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6405 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6388 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6372 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6355 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6338 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6321 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6304 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6287 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6270 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6252 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6235 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6217 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6199 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6182 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6164 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6146 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6127 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6109 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6091 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6072 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.6054 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.fit(X, y, epochs=50, verbose=1)\n",
    "# evaluate the model\n",
    "\n",
    "loss, accuracy = embedding_model.evaluate(X, y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the estimated word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horrible: [ 0.06484096  0.06660457  0.02772433 -0.06045873  0.01426301]\n",
      "Poor: [ 0.04897718  0.05049273  0.09752455 -0.00280119  0.0686097 ]\n",
      "Nice: [-0.02680342 -0.037039   -0.09945454  0.00261691 -0.06602365]\n",
      "Amazing: [-0.09734542 -0.02814195 -0.06347525  0.018133   -0.08245863]\n"
     ]
    }
   ],
   "source": [
    "weights =embedding_model.get_layer('embedding').get_weights()[0]\n",
    "#Horrible\n",
    "print(f'Horrible: {weights[padded_reviews[5][0]]}')\n",
    "#Poor\n",
    "print(f'Poor: {weights[padded_reviews[7][0]]}')\n",
    "#Good\n",
    "print(f'Nice: {weights[padded_reviews[0][0]]}')\n",
    "#Amazing\n",
    "print(f'Amazing: {weights[padded_reviews[1][0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A recurrent model with an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         128000    \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 165,898\n",
      "Trainable params: 165,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=1000, output_dim=128),\n",
    "    tf.keras.layers.GRU(64),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A convolutional recurrent model for sequence \n",
    "- Not a part of Christians lecture..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN for predicting - generating text\n",
    "- Important apply for exercise 5?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlfall23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
