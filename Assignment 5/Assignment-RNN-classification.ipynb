{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - optimization and regularization\n",
    "\n",
    "Hi there! In this assignment, you will use review data to build an RNN model that is able to predict the overall rating of a product from the review text. You are welcome also to use the summary text for this task i you ind it usefull\n",
    "\n",
    "To get you started, I have provided a complete working example.\n",
    "\n",
    "When you are done, submit your results on the Kaggle webpage for this competition. If you do not like to show your score to everyone, you can use an anonymous username on Kaggle.\n",
    "\n",
    "However, I suggest you use your real name, after all it is just meant as an exercise and it is more fun that way. You can submit 5 times every day, so you can experiment with some stuff without being \"locked in\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Data: \n",
    "\n",
    "### y_train.npy:\n",
    "- **overall**: The overall rating (1-5).\n",
    " \n",
    "### X_train.npy and X_test.npy::\n",
    "- **reviewerID**: The ID of the reviewer.\n",
    "- **reviewText**: The text of the review.\n",
    "- **summary**: The summary of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_train =  np.load('X_train.npy',allow_pickle=True)\n",
    "y_train =  np.load('y_train.npy',allow_pickle=True)\n",
    "x_test =  np.load('X_test.npy',allow_pickle=True)\n",
    "\n",
    "df_Xtrain = pd.DataFrame(X_train,columns=['reviewerID','reviewText','summary'])\n",
    "df_ytrain = pd.DataFrame(y_train,columns=['overall'])\n",
    "df_train = pd.concat([df_ytrain, df_Xtrain], axis=1)\n",
    "df_Xtest = pd.DataFrame(x_test,columns=['reviewerID','reviewText','summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review = One of my favorite perfumes and the fact that it is unisex is awesome. I'm gifting this for my nephew.\n",
      "First review has length = 102\n",
      " \n",
      "First review summary= One of my favorite perfumes and the fact that it is unisex is ...\n",
      "First review summary has length = 65\n",
      " \n",
      "First review overall rating = 4\n"
     ]
    }
   ],
   "source": [
    "print(f'First review = {df_train.loc[0, \"reviewText\"]}')\n",
    "print(f'First review has length = {len(df_train.loc[0, \"reviewText\"])}\\n ')\n",
    "print(f'First review summary= {df_train.loc[0, \"summary\"]}')\n",
    "print(f'First review summary has length = {len(df_train.loc[0, \"summary\"])}\\n ')\n",
    "\n",
    "print(f'First review overall rating = {df_train.loc[0, \"overall\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the textual data ready for the RNN model is a bit more involved than for the previous models. We will use the Keras Tokenizer to do most of the work for us. The Tokenizer will split the text into words for us, and create a vocabulary with an index number for each word. We can then represent each text sample by the index numbers of the words in the text. See lecture notes for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "max_tokens = 1000 # the maximum number of words to keep, based on word frequency. Only the most common `max_tokens-1` words will be kept.\n",
    "output_sequence_length = 100 # the maximum length of the sequence to keep. Sequences longer than this will be truncated.\n",
    "pad_to_max_tokens = True # whether to pad to the `output_sequence_length`.\n",
    "\n",
    "# Ensure the text column is of string type and handle NaN values\n",
    "df_train['overall'] = df_train['overall'] - 1 # to make it 0-4\n",
    "df_train['reviewText'] = df_train['reviewText'].fillna('').astype(str)\n",
    "\n",
    "# Initialize the TextVectorization layer\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=max_tokens, output_sequence_length=output_sequence_length, pad_to_max_tokens=pad_to_max_tokens)\n",
    "\n",
    "# Create a dataset of only text data to adapt the encoder\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(df_train['reviewText']).batch(128)\n",
    "encoder.adapt(text_ds)\n",
    "vocab = np.array(encoder.get_vocabulary()) \n",
    "\n",
    "# Create the full train dataset with text and labels\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((df_train['reviewText'], df_train['overall'])).batch(128)\n",
    "# Apply TextVectorization to the text data in the dataset\n",
    "train_ds = train_ds.map(lambda x, y: (encoder(x), y))\n",
    "\n",
    "# Configure the dataset for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the test data ready as we did or the training data\n",
    "df_Xtest['reviewText'] = df_Xtest['reviewText'].fillna('').astype(str)\n",
    "\n",
    "# Convert the texts to sequences using the already adapted TextVectorization layer\n",
    "text_test_ds = tf.data.Dataset.from_tensor_slices(df_Xtest['reviewText']).batch(128)\n",
    "test_ds = text_test_ds.map(lambda x: encoder(x))\n",
    "\n",
    "# Configure the dataset for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [ 47  11  10 277 822   5   2 506  13   6   9   1   9 635  69   1   8  12\n",
      "  10   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "Label: 3\n",
      "---\n",
      "Review: [  3  82   1 111   2 232   1  96   5 352  21  48  21   2 248  96   5 352\n",
      "   3  19  59   8 211 743  14 908  17   2 248 352 686   5   1  21  48  21\n",
      " 116 402   3 177  13   6 827   2 149 104  17 220  21 201  21   2  73 216\n",
      " 459  71   9   7 141  13 577  19  41   2  96  22 120  14   2 104 624 556\n",
      " 938 622  13  22  49  19   7   1  39 149 104  12   4 131 285   7 112   8\n",
      " 986   3 177  13   1   2  96  27  11   1]\n",
      "Label: 4\n",
      "---\n",
      "Review: [  1  56   6 145  15   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "Label: 3\n",
      "---\n",
      "Review: [296 197 140 884  17   1  58 296   3  50   8   5  25  49  22   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "Label: 4\n",
      "---\n",
      "Review: [ 43  33  10   1   1 707   6   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "Label: 4\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first few batches of the train_ds\n",
    "for text_batch, label_batch in train_ds.take(1):  # Adjust .take() for more batches\n",
    "    for i in range(5):  # Adjust the range to see more or fewer examples\n",
    "        print(\"Review:\", text_batch.numpy()[i])\n",
    "        print(\"Label:\", label_batch.numpy()[i])\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 128\n",
    "embedding_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(vocab), \n",
    "                              output_dim=embedding_dimension,\n",
    "                              input_length=100,\n",
    "                              name=\"embedding\"), \n",
    "    tf.keras.layers.LSTM(128),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 128)          128000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 260,229\n",
      "Trainable params: 260,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 51s 207ms/step - loss: 1.1282 - accuracy: 0.5889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20320d85f00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.fit(train_ds, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 2s 59ms/step\n",
      "[5 5 5 ... 5 5 4]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = embedding_model.predict(test_ds)\n",
    "# The 'predictions' array will contain the probabilities of each class for each sample\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)+1\n",
    "\n",
    "# 'predicted_labels' now contains the class label (1 to 5) for each sample in your test dataset\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_pd = pd.DataFrame({\n",
    "    'Id': list(range(len(predicted_labels))),\n",
    "    'Predicted': predicted_labels.reshape(-1),\n",
    "})\n",
    "y_test_hat_pd.to_csv('y_test_hat.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
